{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "import ast\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/fine_tuning_util.ipynb\n",
    "%run ../utils/save_and_load_util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"eda_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How does the Supreme Court differentiate between an interlocutory order and a final order in civil appeals?', 'What is the legal significance of the immunity granted to Attorneys-at-Law regarding statements made in pleadings?', 'In tax disputes, how should conflicts between two statutes be resolved?', 'How does Sri Lankan law determine the â€˜valueâ€™ of shares for stamp duty purposes?']\n"
     ]
    }
   ],
   "source": [
    "# Load the true questions and answers\n",
    "qa_data = pd.read_csv(\"../data/processed/Questions & Answers.csv\")\n",
    "questions = qa_data[\"question\"].tolist()\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Text for Training & Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Using OpenAI's tokenizer\n",
    "max_chunk_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:   5%|â–Œ         | 157/2988 [00:02<00:27, 101.36file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_ba_43_2011.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  12%|â–ˆâ–        | 347/2988 [00:03<00:20, 130.85file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_dc_horana_563_96_f.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  14%|â–ˆâ–        | 433/2988 [00:04<00:19, 130.54file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_dc_matara_05_97.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  16%|â–ˆâ–‹        | 493/2988 [00:05<00:19, 130.11file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_dc_panadura_416_92.pdf due to invalid text.\n",
      "Skipping file ca_dc_ratnapura_646_99.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  22%|â–ˆâ–ˆâ–       | 669/2988 [00:06<00:09, 237.09file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_pa_26_2011.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  26%|â–ˆâ–ˆâ–Œ       | 769/2988 [00:06<00:07, 285.31file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_phc_apn_27_2012.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  29%|â–ˆâ–ˆâ–‰       | 880/2988 [00:07<00:09, 213.17file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_writ_1591_06.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 1011/2988 [00:07<00:10, 190.68file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file ca_writ_603_10.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2910/2988 [00:58<00:01, 47.48file/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file sc_hc_la_89_2022.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2988/2988 [01:00<00:00, 49.53file/s, remaining=0.00s]\n"
     ]
    }
   ],
   "source": [
    "chunked_data = []\n",
    "total_files = len(df)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=total_files, desc=\"Chunking Progress\", unit=\"file\") as pbar:\n",
    "    for _, row in df.iterrows():\n",
    "        file_name = row[\"file_name\"]\n",
    "        \n",
    "        # Ensure 'text' is a valid string\n",
    "        text = row[\"text\"]\n",
    "        if isinstance(text, str):  # Proceed only if text is a string\n",
    "            chunks = chunk_text(text)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_data.append({\"file_name\": file_name, \"chunk_id\": i, \"text\": chunk})\n",
    "        else:\n",
    "            print(f\"Skipping file {file_name} due to invalid text.\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Calculate time left and show it dynamically\n",
    "    elapsed_time = time.time() - start_time\n",
    "    remaining_time = (elapsed_time / pbar.n) * (total_files - pbar.n)\n",
    "    pbar.set_postfix(remaining=f\"{remaining_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "chunked_df = pd.DataFrame(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the chunked data\n",
    "chunked_df.to_csv(\"chunked_law_cases.csv\", index=False)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_df = pd.read_csv(\"chunked_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert questions to embeddings\n",
    "query_vectors = np.array(model.encode(questions, convert_to_numpy=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "d = 384\n",
    "index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 35008chunk [1:00:30,  9.64chunk/s]                        \n"
     ]
    }
   ],
   "source": [
    "chunked_data_with_embeddings = []\n",
    "total_chunks = len(chunked_df)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=total_chunks, desc=\"Generating Embeddings\", unit=\"chunk\") as pbar:\n",
    "    batch_size = 32  # Adjust the batch size based on available memory\n",
    "\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch_texts = chunked_df[\"text\"].iloc[i:i + batch_size].tolist()\n",
    "        embeddings = generate_embeddings(batch_texts)\n",
    "\n",
    "        # Add embeddings to the dataframe\n",
    "        for j, emb in enumerate(embeddings):\n",
    "            chunked_data_with_embeddings.append({\n",
    "                \"file_name\": chunked_df[\"file_name\"].iloc[i + j],\n",
    "                \"chunk_id\": chunked_df[\"chunk_id\"].iloc[i + j],\n",
    "                \"text\": chunked_df[\"text\"].iloc[i + j],\n",
    "                \"embedding\": emb.cpu().numpy()\n",
    "            })\n",
    "        \n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the chunked data with embeddings to a DataFrame\n",
    "embedding_df = pd.DataFrame(chunked_data_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to disk\n",
    "embedding_df.to_csv(\"law_cases_with_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunked data with embeddings\n",
    "embedding_df = pd.read_csv(\"law_cases_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string embeddings back to NumPy arrays\n",
    "def parse_embedding(embedding_str):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(embedding_str), dtype=np.float32)\n",
    "    except:\n",
    "        return np.zeros(384, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df[\"embedding\"] = embedding_df[\"embedding\"].apply(parse_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to a 2D NumPy array\n",
    "embeddings_matrix = np.vstack(embedding_df[\"embedding\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FAISS index (L2 Normalized for cosine similarity search)\n",
    "embedding_dim = embeddings_matrix.shape[1]  # Get the embedding dimension\n",
    "index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to FAISS index\n",
    "index.add(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "faiss.write_index(index, \"law_cases_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata (file names & chunk IDs) for retrieval\n",
    "metadata = embedding_df[[\"file_name\", \"chunk_id\", \"text\"]].to_dict(orient=\"records\")\n",
    "with open(\"faiss_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "index = faiss.read_index(\"law_cases_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata (file names & text chunks)\n",
    "with open(\"faiss_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Same model used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_cases(query, top_k=5):\n",
    "    # Convert query into an embedding\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding = np.array(query_embedding, dtype=np.float32)  \n",
    "\n",
    "    # Search FAISS for top 10 matches (to get diversity)\n",
    "    distances, indices = index.search(query_embedding, top_k * 3)  \n",
    "\n",
    "    # Keep track of unique documents\n",
    "    unique_cases = {}\n",
    "    \n",
    "    for i in range(len(indices[0])):\n",
    "        idx = indices[0][i]\n",
    "        if idx < len(metadata):  # Ensure index is valid\n",
    "            case = metadata[idx]\n",
    "            file_name = case[\"file_name\"]\n",
    "            \n",
    "            if file_name not in unique_cases:  # Add only if not already included\n",
    "                unique_cases[file_name] = case\n",
    "\n",
    "            if len(unique_cases) == top_k:  # Stop when we have top_k unique documents\n",
    "                break\n",
    "\n",
    "    return list(unique_cases.values())  # Return only unique cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform FAISS search for the questions\n",
    "k = 1  # Get only the top result\n",
    "_, ground_truth_indices = index.search(query_vectors, k)\n",
    "\n",
    "# Flatten the array since FAISS returns a list of lists\n",
    "ground_truth_indices = ground_truth_indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faiss_index(index, query_vectors, ground_truth_indices, k=5):\n",
    "    \"\"\"Evaluates FAISS retrieval performance\"\"\"\n",
    "    start_time = time.time()\n",
    "    cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "    \n",
    "    _, retrieved_indices = index.search(query_vectors, k)\n",
    "    \n",
    "    cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    # Calculate accuracy (percentage of ground truth indices found in retrieved indices)\n",
    "    correct_retrievals = sum([1 for gt, retrieved in zip(ground_truth_indices, retrieved_indices) if gt in retrieved])\n",
    "    accuracy = correct_retrievals / len(ground_truth_indices)\n",
    "\n",
    "    print(f\"FAISS Evaluation:\\n- Response Time: {response_time:.4f}s\\n- CPU Usage: {cpu_usage_after - cpu_usage_before:.2f}%\\n- Accuracy: {accuracy:.2%}\")\n",
    "    return response_time, cpu_usage_after - cpu_usage_before, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Evaluation:\n",
      "- Response Time: 0.0094s\n",
      "- CPU Usage: 47.30%\n",
      "- Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "response_time, cpu_usage, accuracy = evaluate_faiss_index(index, query_vectors, ground_truth_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation With Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_llama(query):\n",
    "    \"\"\"\n",
    "    Given a legal query, retrieve relevant law cases and generate a response using Llama 3.1.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant legal cases\n",
    "    relevant_cases = retrieve_relevant_cases(query)\n",
    "\n",
    "    # Combine case texts\n",
    "    case_texts = \"\\n\\n\".join([f\"Case {i+1}: {case['text']}\" for i, case in enumerate(relevant_cases)])\n",
    "\n",
    "    # Construct Llama 3.1 prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a legal AI assistant. Answer the query based on the following legal cases:\n",
    "\n",
    "    {case_texts}\n",
    "\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get response from Llama 3.1\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return response, relevant_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "qa_data = pd.read_csv(\"../data/processed/Questions & Answers.csv\")\n",
    "questions = qa_data[\"question\"].tolist()\n",
    "ground_truth_answers = qa_data[\"answer\"].tolist()  # True legal answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize empty lists\n",
    "llm_responses = []\n",
    "retrieved_cases = []\n",
    "\n",
    "# Monitor system resource usage\n",
    "cpu_usage_list = []\n",
    "memory_usage_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [21:24<1:04:13, 1284.46s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed 1/4 | Time: 1284.46s | CPU: 33.15% | Memory: 82.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [34:36<33:09, 994.63s/question]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed 2/4 | Time: 791.71s | CPU: 40.85% | Memory: 87.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [43:12<12:56, 776.24s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed 3/4 | Time: 516.35s | CPU: 43.30% | Memory: 86.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [53:53<00:00, 808.30s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed 4/4 | Time: 640.64s | CPU: 33.00% | Memory: 86.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each question with a progress bar\n",
    "for i, question in enumerate(tqdm(questions, desc=\"Processing Questions\", unit=\"question\")):\n",
    "    question_start_time = time.time()  # Track time for each question\n",
    "\n",
    "    # Record CPU & memory usage before processing\n",
    "    cpu_before = psutil.cpu_percent(interval=None)\n",
    "    memory_before = psutil.virtual_memory().percent\n",
    "\n",
    "    # Generate response using LLaMA\n",
    "    response, matched_cases = generate_response_with_llama(question)\n",
    "    \n",
    "    # Store results\n",
    "    llm_responses.append(response)\n",
    "    retrieved_cases.append(matched_cases)\n",
    "\n",
    "    # Record CPU & memory usage after processing\n",
    "    cpu_after = psutil.cpu_percent(interval=None)\n",
    "    memory_after = psutil.virtual_memory().percent\n",
    "\n",
    "    # Compute stats\n",
    "    avg_cpu = (cpu_before + cpu_after) / 2\n",
    "    avg_memory = (memory_before + memory_after) / 2\n",
    "    elapsed_time_per_question = time.time() - question_start_time\n",
    "\n",
    "    # Append to lists\n",
    "    cpu_usage_list.append(avg_cpu)\n",
    "    memory_usage_list.append(avg_memory)\n",
    "\n",
    "    # Print log after every question\n",
    "    print(f\"âœ… Completed {i + 1}/{len(questions)} | Time: {elapsed_time_per_question:.2f}s | \"\n",
    "          f\"CPU: {avg_cpu:.2f}% | Memory: {avg_memory:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All 4 questions processed in 3680.47 seconds.\n",
      "ðŸ“Š Final Average CPU Usage: 37.58%\n",
      "ðŸ“Š Final Average Memory Usage: 86.00%\n"
     ]
    }
   ],
   "source": [
    "# Final execution time\n",
    "total_time = time.time() - start_time\n",
    "final_cpu_usage = sum(cpu_usage_list) / len(cpu_usage_list)\n",
    "final_memory_usage = sum(memory_usage_list) / len(memory_usage_list)\n",
    "\n",
    "print(f\"\\nâœ… All {len(questions)} questions processed in {total_time:.2f} seconds.\")\n",
    "print(f\"ðŸ“Š Final Average CPU Usage: {final_cpu_usage:.2f}%\")\n",
    "print(f\"ðŸ“Š Final Average Memory Usage: {final_memory_usage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ROUGE for text comparison\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA Response Evaluation: {'rouge1': 0.21996053611837663, 'rouge2': 0.06654455294784305, 'rougeL': 0.14734921260779954, 'rougeLsum': 0.16019937901302683}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate responses\n",
    "llm_evaluation = rouge.compute(predictions=llm_responses, references=ground_truth_answers)\n",
    "\n",
    "print(\"LLaMA Response Evaluation:\", llm_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1601036b760c43d6b13cc2be4685d868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db166299ca13420aac1fd85c778883b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9bb2dc8a004c0fba0bdb8e018bd19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892d90b00c7b4d938923bdc679d5a840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8174ed55dd4ccb8a55053fd4ec31a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c5a813ee2a4545bc608beddc7ad818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore: {'precision': [0.8117244243621826, 0.8116472959518433, 0.7999582290649414, 0.7952038645744324], 'recall': [0.8550233840942383, 0.863749623298645, 0.8507528901100159, 0.8673533201217651], 'f1': [0.8328114748001099, 0.836888313293457, 0.8245741128921509, 0.8297131061553955], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.45.2)'}\n"
     ]
    }
   ],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bert_evaluation = bertscore.compute(predictions=llm_responses, references=ground_truth_answers, lang=\"en\")\n",
    "\n",
    "print(\"BERTScore:\", bert_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_query = \"What are the legal rights of a tenant in a lease agreement dispute?\"\n",
    "response, matched_cases = generate_response_with_llama(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”· AI Legal Assistant Response:\n",
      "\n",
      "Based on general principles of law and the cases provided, I'll address the query. Please note that specific provisions may vary depending on jurisdiction (in this case, Sri Lanka).\n",
      "\n",
      "In general, when it comes to a lease agreement dispute, the rights of a tenant can be summarized as follows:\n",
      "\n",
      "1. **Right to Quiet Possession**: The tenant has the right to peaceful enjoyment of the property during the tenancy period. This means they should not be disturbed or evicted without proper notice and due process.\n",
      "2. **Right to Repairs and Maintenance**: The landlord is responsible for maintaining the property in a habitable condition. If repairs are necessary, the tenant can request the landlord to take action.\n",
      "3. **Right to Receive Rent Payments**: The tenant has the right to receive rent payments as agreed upon in the lease agreement. However, if there's a dispute over rent or late payment fees, the tenant may seek mediation or legal recourse.\n",
      "4. **Right to Terminate Lease**: If the landlord breaches the terms of the lease (e.g., fails to provide essential services), the tenant can terminate the lease without penalty.\n",
      "\n",
      "In Case 1 (SC Application Special Vs. [Expulsion] No. 01/2009), the petitioner, Ameer Ali Shihabdeen, was a member of the Sri Lanka Muslim Congress and had been elected as a Member of Parliament. While this case doesn't directly address tenant rights in lease disputes, it highlights the importance of adherence to constitutional procedures and due process.\n",
      "\n",
      "Case 2 (SC/TAB No. 01A-01F/2017) is a criminal appeal case involving six accused individuals. This case does not pertain to lease agreements or tenant rights.\n",
      "\n",
      "Considering these cases and general principles of law, I can provide the following answer:\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "In Sri Lanka, a tenant in a lease agreement dispute has the right to peaceful possession (quiet enjoyment), repairs and maintenance from the landlord, receive rent payments as agreed upon, and terminate the lease if there's a breach by the landlord. If a tenant is facing issues related to their lease, they can seek mediation or legal recourse through relevant authorities.\n",
      "\n",
      "Please note that this answer may not be exhaustive and might require further clarification based on specific circumstances or local regulations.\n"
     ]
    }
   ],
   "source": [
    "# Print response\n",
    "print(\"ðŸ”· AI Legal Assistant Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”· Top Matching Case Files:\n",
      "ðŸ“„ 012009.pdf\n",
      "ðŸ“„ 01a_01f_2017_tab.pdf\n"
     ]
    }
   ],
   "source": [
    "# Print matched case file names\n",
    "print(\"\\nðŸ”· Top Matching Case Files:\")\n",
    "for case in matched_cases:\n",
    "    print(f\"ðŸ“„ {case['file_name']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
