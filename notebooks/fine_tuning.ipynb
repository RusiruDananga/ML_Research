{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/fine_tuning_util.ipynb\n",
    "%run ../utils/save_and_load_util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"eda_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Text for Training & Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Using OpenAI's tokenizer\n",
    "max_chunk_size = 512  # Choose chunk size based on LLaMA's context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1833/1910 [00:48<00:01, 51.40file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file sc_hc_la_89_2022.pdf due to invalid text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1910/1910 [00:50<00:00, 37.70file/s, remaining=0.00s]\n"
     ]
    }
   ],
   "source": [
    "# Apply chunking with a single progress bar\n",
    "chunked_data = []\n",
    "total_files = len(df)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Using tqdm to show progress bar for files\n",
    "with tqdm(total=total_files, desc=\"Chunking Progress\", unit=\"file\") as pbar:\n",
    "    for _, row in df.iterrows():\n",
    "        file_name = row[\"file_name\"]\n",
    "        \n",
    "        # Ensure 'text' is a valid string\n",
    "        text = row[\"text\"]\n",
    "        if isinstance(text, str):  # Proceed only if text is a string\n",
    "            chunks = chunk_text(text)\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_data.append({\"file_name\": file_name, \"chunk_id\": i, \"text\": chunk})\n",
    "        else:\n",
    "            print(f\"Skipping file {file_name} due to invalid text.\")\n",
    "        \n",
    "        # Update the progress bar after each file\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Calculate time left and show it dynamically\n",
    "    elapsed_time = time.time() - start_time\n",
    "    remaining_time = (elapsed_time / pbar.n) * (total_files - pbar.n)\n",
    "    pbar.set_postfix(remaining=f\"{remaining_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "chunked_df = pd.DataFrame(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the chunked data\n",
    "chunked_df.to_csv(\"chunked_law_cases.csv\", index=False)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_df = pd.read_csv(\"chunked_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index with correct dimensionality\n",
    "d = 384  # Match the SentenceTransformer output\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings:   3%|â–Ž         | 896/30646 [01:20<44:51, 11.05chunk/s] "
     ]
    }
   ],
   "source": [
    "# Apply embedding generation with progress bar\n",
    "chunked_data_with_embeddings = []\n",
    "total_chunks = len(chunked_df)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Using tqdm for progress bar\n",
    "with tqdm(total=total_chunks, desc=\"Generating Embeddings\", unit=\"chunk\") as pbar:\n",
    "    batch_size = 32  # Adjust the batch size based on available memory\n",
    "\n",
    "    # Process in batches for better performance\n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch_texts = chunked_df[\"text\"].iloc[i:i + batch_size].tolist()\n",
    "        embeddings = generate_embeddings(batch_texts)\n",
    "\n",
    "        # Add embeddings to the dataframe\n",
    "        for j, emb in enumerate(embeddings):\n",
    "            chunked_data_with_embeddings.append({\n",
    "                \"file_name\": chunked_df[\"file_name\"].iloc[i + j],\n",
    "                \"chunk_id\": chunked_df[\"chunk_id\"].iloc[i + j],\n",
    "                \"text\": chunked_df[\"text\"].iloc[i + j],\n",
    "                \"embedding\": emb.cpu().numpy()  # Convert tensor to numpy array\n",
    "            })\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the chunked data with embeddings to a DataFrame\n",
    "embedding_df = pd.DataFrame(chunked_data_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the embeddings to disk\n",
    "embedding_df.to_csv(\"law_cases_with_embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunked data with embeddings\n",
    "embedding_df = pd.read_csv(\"law_cases_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string embeddings back to NumPy arrays\n",
    "def parse_embedding(embedding_str):\n",
    "    try:\n",
    "        return np.array(ast.literal_eval(embedding_str), dtype=np.float32)  # Ensures float32 format\n",
    "    except:\n",
    "        return np.zeros(384, dtype=np.float32)  # Use zero vector for any malformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df[\"embedding\"] = embedding_df[\"embedding\"].apply(parse_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to a 2D NumPy array\n",
    "embeddings_matrix = np.vstack(embedding_df[\"embedding\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FAISS index (L2 Normalized for cosine similarity search)\n",
    "embedding_dim = embeddings_matrix.shape[1]  # Get the embedding dimension\n",
    "index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to FAISS index\n",
    "index.add(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "faiss.write_index(index, \"law_cases_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata (file names & chunk IDs) for retrieval\n",
    "metadata = embedding_df[[\"file_name\", \"chunk_id\", \"text\"]].to_dict(orient=\"records\")\n",
    "with open(\"faiss_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "index = faiss.read_index(\"law_cases_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata (file names & text chunks)\n",
    "with open(\"faiss_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Same model used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_cases(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Given a legal query, find the most relevant law case chunks using FAISS.\n",
    "    \"\"\"\n",
    "    # Convert query to embedding\n",
    "    query_embedding = embedding_model.encode([query])  # Convert query to embedding\n",
    "    query_embedding = np.array(query_embedding, dtype=np.float32)  # Ensure float32 format\n",
    "\n",
    "    # Search FAISS index for closest matches\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Retrieve the top matching cases\n",
    "    retrieved_cases = []\n",
    "    for i in range(top_k):\n",
    "        if indices[0][i] < len(metadata):  # Ensure index is within bounds\n",
    "            retrieved_cases.append(metadata[indices[0][i]])\n",
    "\n",
    "    return retrieved_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_llama(query):\n",
    "    \"\"\"\n",
    "    Given a legal query, retrieve relevant law cases and generate a response using Llama 3.1.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant legal cases\n",
    "    relevant_cases = retrieve_relevant_cases(query)\n",
    "\n",
    "    # Combine case texts\n",
    "    case_texts = \"\\n\\n\".join([f\"Case {i+1}: {case['text']}\" for i, case in enumerate(relevant_cases)])\n",
    "\n",
    "    # Construct Llama 3.1 prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a legal AI assistant. Answer the query based on the following legal cases:\n",
    "\n",
    "    {case_texts}\n",
    "\n",
    "    Query: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get response from Llama 3.1\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return response, relevant_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_query = \"What are the legal rights of a tenant in a lease agreement dispute?\"\n",
    "response, matched_cases = generate_response_with_llama(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”· AI Legal Assistant Response:\n",
      "\n",
      "It appears that you have provided a lengthy document related to a court case or dispute between individuals, rather than asking about the legal rights of a tenant in a lease agreement dispute.\n",
      "\n",
      "To provide an answer to your original question, here are some general legal rights that a tenant may have in a lease agreement dispute:\n",
      "\n",
      "1. Right to quiet enjoyment: A tenant has the right to occupy and use the rented property without interference or disturbance from the landlord.\n",
      "2. Right to notice of termination: A tenant is entitled to written notice before their tenancy can be terminated, unless the lease agreement specifies otherwise.\n",
      "3. Right to security deposit refund: At the end of a tenancy, a tenant may be entitled to a refund of their security deposit, minus any damages or deductions allowed by law.\n",
      "4. Right to inspection and repairs: A tenant has the right to inspect and report on any necessary repairs, and to request that they be made in a timely manner.\n",
      "5. Right to termination notice requirements: In some jurisdictions, tenants have specific rights regarding the timing and content of termination notices.\n",
      "\n",
      "These are general examples, and the specific legal rights of a tenant can vary depending on the jurisdiction, lease agreement terms, and local laws. If you would like more information or clarification on this topic, please let me know!\n"
     ]
    }
   ],
   "source": [
    "# Print response\n",
    "print(\"ðŸ”· AI Legal Assistant Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”· Top Matching Case Files:\n",
      "ðŸ“„ 012009.pdf\n",
      "ðŸ“„ 012009.pdf\n",
      "ðŸ“„ 012009.pdf\n",
      "ðŸ“„ 012009.pdf\n",
      "ðŸ“„ 012009.pdf\n"
     ]
    }
   ],
   "source": [
    "# Print matched case file names\n",
    "print(\"\\nðŸ”· Top Matching Case Files:\")\n",
    "for case in matched_cases:\n",
    "    print(f\"ðŸ“„ {case['file_name']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
