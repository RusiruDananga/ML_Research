{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "import ast\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import psutil\n",
    "from bert_score import score\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/fine_tuning_util.ipynb\n",
    "%run ../utils/save_and_load_util.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient embedding model\n",
    "chunked_df = pd.read_csv(\"src/chunked_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_df['embeddings'] = chunked_df['text'].apply(lambda x: model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in chunked_df.iterrows():\n",
    "    G.add_node(row['chunk_id'], text=row['text'])\n",
    "    if _ > 0:\n",
    "        G.add_edge(row['chunk_id'], row['chunk_id'] - 1)  # Sequential linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(chunked_df['embeddings'].tolist()).astype('float32')\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar(query):\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    D, I = index.search(query_embedding, 5)  # Retrieve top 5 similar chunks\n",
    "    return chunked_df.iloc[I[0]]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The legal precedent for contract breaches in this case is that a party who breaks a contract does not discharge the other party's obligations, but rather gives the injured party the option to either:\n",
      "\n",
      "1. Compel the guilty party to perform their promise (specific performance)\n",
      "2. Sue them for damages\n",
      "\n",
      "In this specific case, the Plaintiff continued to offer payment of the balance consideration and kept the contract \"alive\" beyond the period of three months, indicating that they did not accept the anticipatory breach by the 1st Defendant as a discharge of the obligations of the agreement to sell.\n",
      "\n",
      "Therefore, the legal precedent is that even a breach sufficient to effect a discharge does not itself discharge the contract, but merely gives the other party an option to decide whether they will treat the contract as discharged.\n"
     ]
    }
   ],
   "source": [
    "def query_llama(query):\n",
    "    relevant_chunks = retrieve_similar(query)\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "\n",
    "    response = ollama.chat(model='llama3.1', messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a legal AI assistant trained on case law.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Use this context:\\n{context}\\n\\nQuestion: {query}\"}\n",
    "    ])\n",
    "    return response['message']['content']\n",
    "\n",
    "print(query_llama(\"What is the legal precedent for contract breaches?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_graph_rag(query):\n",
    "    similar_chunks = retrieve_similar(query)\n",
    "    expanded_context = set(similar_chunks)\n",
    "\n",
    "    for chunk in similar_chunks:\n",
    "        neighbors = G.neighbors(chunked_df[chunked_df['text'] == chunk]['chunk_id'].values[0])\n",
    "        expanded_context.update([G.nodes[n]['text'] for n in neighbors])\n",
    "\n",
    "    return \"\\n\".join(expanded_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llama_graph_rag(query):\n",
    "    context = expand_graph_rag(query)\n",
    "\n",
    "    response = ollama.chat(model='llama3.1', messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a legal AI assistant trained on case law using Graph RAG.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Use this context:\\n{context}\\n\\nQuestion: {query}\"}\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def measure_performance(query, answer):\n",
    "    start_time = time.time()\n",
    "    cpu_before = psutil.cpu_percent(interval=None)\n",
    "    memory_before = psutil.virtual_memory().percent\n",
    "    \n",
    "    generated_answer = query_llama_graph_rag(query)\n",
    "    \n",
    "    cpu_after = psutil.cpu_percent(interval=None)\n",
    "    memory_after = psutil.virtual_memory().percent\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response_time = end_time - start_time\n",
    "    avg_cpu_usage = (cpu_before + cpu_after) / 2\n",
    "    avg_memory_usage = (memory_before + memory_after) / 2\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_answer, answer)[0]\n",
    "    \n",
    "    P, R, F1 = score([generated_answer], [answer], lang=\"en\")\n",
    "    bert_score = F1.mean().item()\n",
    "    \n",
    "    return response_time, avg_cpu_usage, avg_memory_usage, rouge_scores, bert_score, generated_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "eval_df = pd.read_csv(\"../data/processed/Questions & Answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "cpu_usage_list = []\n",
    "memory_usage_list = []\n",
    "llm_responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions:   0%|          | 0/4 [00:00<?, ?question/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing Questions:  25%|██▌       | 1/4 [34:36<1:43:49, 2076.58s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed 1/4 | Time: 2069.73s | CPU: 34.50% | Memory: 72.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing Questions:  50%|█████     | 2/4 [1:24:02<1:26:39, 2599.69s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed 2/4 | Time: 2959.34s | CPU: 55.20% | Memory: 91.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing Questions:  75%|███████▌  | 3/4 [2:12:14<45:33, 2733.24s/question]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed 3/4 | Time: 2881.47s | CPU: 58.70% | Memory: 89.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing Questions: 100%|██████████| 4/4 [2:42:17<00:00, 2434.27s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed 4/4 | Time: 1798.56s | CPU: 59.05% | Memory: 87.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Processing Questions\", unit=\"question\"):\n",
    "    response_time, avg_cpu_usage, avg_memory_usage, rouge_scores, bert_score, generated_answer = measure_performance(row['question'], row['answer'])\n",
    "    \n",
    "    result = {\n",
    "        \"question\": row['question'],\n",
    "        \"response\": generated_answer,\n",
    "        \"response_time\": response_time,\n",
    "        \"cpu_usage\": avg_cpu_usage,\n",
    "        \"memory_usage\": avg_memory_usage,\n",
    "        \"rouge_scores\": rouge_scores,\n",
    "        \"bert_score\": bert_score\n",
    "    }\n",
    "    results.append(result)\n",
    "    cpu_usage_list.append(avg_cpu_usage)\n",
    "    memory_usage_list.append(avg_memory_usage)\n",
    "    llm_responses.append(generated_answer)\n",
    "    \n",
    "    print(f\"✅ Completed {i + 1}/{len(eval_df)} | Time: {response_time:.2f}s | CPU: {avg_cpu_usage:.2f}% | Memory: {avg_memory_usage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute final stats\n",
    "total_time = time.time() - start_time\n",
    "final_cpu_usage = sum(cpu_usage_list) / len(cpu_usage_list)\n",
    "final_memory_usage = sum(memory_usage_list) / len(memory_usage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ROUGE score keys\n",
    "rouge_keys = results[0][\"rouge_scores\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average ROUGE scores\n",
    "avg_rouge = {\n",
    "    k: sum(r[\"rouge_scores\"][k][\"f\"] for r in results) / len(results) for k in rouge_keys\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average BERTScore\n",
    "avg_bert_score = sum(r[\"bert_score\"] for r in results) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(results[0][\"bert_score\"], float):\n",
    "    # If it's a float, assume it's the F1 score and set precision & recall to the same value\n",
    "    avg_bert_precision = avg_bert_recall = avg_bert_f1 = sum(r[\"bert_score\"] for r in results) / len(results)\n",
    "else:\n",
    "    # If it's a dictionary, compute proper averages\n",
    "    avg_bert_precision = sum(r[\"bert_score\"][\"precision\"] for r in results) / len(results)\n",
    "    avg_bert_recall = sum(r[\"bert_score\"][\"recall\"] for r in results) / len(results)\n",
    "    avg_bert_f1 = sum(r[\"bert_score\"][\"f1\"] for r in results) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All 4 questions processed in 14153.64 seconds.\n",
      "📊 Final Average CPU Usage: 51.86%\n",
      "📊 Final Average Memory Usage: 85.11%\n",
      "📊 Final Average ROUGE Scores: {'rouge-1': 0.2978928795317439, 'rouge-2': 0.08846930858822495, 'rouge-l': 0.2780273975007121}\n",
      "📊 Final Average BERTScore: 0.8590720742940903\n",
      "📊 Final BERTScore Precision: 0.8591\n",
      "📊 Final BERTScore Recall: 0.8591\n",
      "📊 Final BERTScore F1-score: 0.8591\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ All {len(eval_df)} questions processed in {total_time:.2f} seconds.\")\n",
    "print(f\"📊 Final Average CPU Usage: {final_cpu_usage:.2f}%\")\n",
    "print(f\"📊 Final Average Memory Usage: {final_memory_usage:.2f}%\")\n",
    "print(f\"📊 Final Average ROUGE Scores: {avg_rouge}\")\n",
    "print(f\"📊 Final Average BERTScore: {avg_bert_score}\")\n",
    "print(f\"📊 Final BERTScore Precision: {avg_bert_precision:.4f}\")\n",
    "print(f\"📊 Final BERTScore Recall: {avg_bert_recall:.4f}\")\n",
    "print(f\"📊 Final BERTScore F1-score: {avg_bert_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
