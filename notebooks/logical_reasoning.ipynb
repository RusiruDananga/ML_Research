{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import spacy\n",
    "from sympy import symbols, Implies\n",
    "import faiss\n",
    "import torch\n",
    "from langchain_ollama import OllamaLLM\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import evaluate\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunked legal cases dataset\n",
    "chunked_cases_df = pd.read_csv(\"chunked_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize graph\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node embeddings\n",
    "chunk_embeddings = model.encode(chunked_cases_df[\"text\"].tolist(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to the graph\n",
    "for index, row in chunked_cases_df.iterrows():\n",
    "    G.add_node(row[\"chunk_id\"], text=row[\"text\"], embedding=chunk_embeddings[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish logical dependencies between chunks\n",
    "for i, row in chunked_cases_df.iterrows():\n",
    "    current_embedding = chunk_embeddings[i].cpu().numpy()\n",
    "    similarities = cosine_similarity([current_embedding], chunk_embeddings.cpu().numpy())[0]\n",
    "    \n",
    "    # Find the top related chunks (excluding itself)\n",
    "    top_related = np.argsort(similarities)[-6:-1]  # Get top 5 similar chunks\n",
    "    \n",
    "    for related_index in top_related:\n",
    "        related_chunk_id = chunked_cases_df.iloc[related_index][\"chunk_id\"]\n",
    "        G.add_edge(row[\"chunk_id\"], related_chunk_id, weight=similarities[related_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save graph using pickle (more suitable for complex objects)\n",
    "# with open(\"../data/processed/logical_reasoning/graph_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Reasoning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_logically_relevant(query, chunk_text):\n",
    "    \"\"\"\n",
    "    Rule-based filtering to check if a chunk is logically relevant.\n",
    "    - Prioritizes legal precedent cases.\n",
    "    - Ensures the chunk discusses related legal principles.\n",
    "    \"\"\"\n",
    "    keywords = [\"precedent\", \"ruling\", \"judgment\", \"legal principle\", \"interpretation\"]\n",
    "    if any(keyword in chunk_text.lower() for keyword in keywords):\n",
    "        return True  # Accept if it contains key legal reasoning terms\n",
    "    return False  # Reject otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to NumPy array\n",
    "embedding_dim = chunk_embeddings.shape[1]  # Get the embedding size\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)  # L2 (Euclidean) distance index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index initialized with 34982 legal case chunks.\n"
     ]
    }
   ],
   "source": [
    "# Convert PyTorch tensor to NumPy and add to FAISS\n",
    "faiss_index.add(chunk_embeddings.cpu().numpy())  # Ensure it's a NumPy array\n",
    "\n",
    "print(\"‚úÖ FAISS index initialized with\", faiss_index.ntotal, \"legal case chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves top_k most similar legal case chunks using FAISS.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy()\n",
    "    \n",
    "    # FAISS similarity search\n",
    "    _, indices = faiss_index.search(query_embedding, top_k)\n",
    "\n",
    "    # Retrieve the actual text of similar chunks\n",
    "    similar_chunks = chunked_cases_df.iloc[indices[0]]['text'].tolist()\n",
    "    \n",
    "    return similar_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_expand_graph_rag(query):\n",
    "    \"\"\"\n",
    "    Expands the context for retrieval by using graph-based reasoning.\n",
    "    - Retrieves top similar chunks.\n",
    "    - Expands context using graph neighbors.\n",
    "    - Applies rule-based filtering for logical relevance.\n",
    "    \"\"\"\n",
    "    similar_chunks = retrieve_similar(query)  # Step 1: Retrieve initial similar chunks\n",
    "    expanded_context = set(similar_chunks)  # Step 2: Store similar chunks\n",
    "\n",
    "    # Step 3: Expand using graph-based logical connections\n",
    "    for chunk in similar_chunks:\n",
    "        chunk_id = chunked_cases_df[chunked_cases_df['text'] == chunk]['chunk_id'].values[0]\n",
    "        neighbors = list(G.neighbors(chunk_id))  # Get connected chunks (logical relations)\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            neighbor_text = G.nodes[neighbor]['text']\n",
    "\n",
    "            # Step 4: Apply symbolic reasoning (filter relevant legal cases)\n",
    "            if is_logically_relevant(query, neighbor_text):  \n",
    "                expanded_context.add(neighbor_text)\n",
    "\n",
    "    return \"\\n\".join(expanded_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logically_reasoned_response(query):\n",
    "    \"\"\"\n",
    "    Generates a response using LLaMA with logically expanded retrieval.\n",
    "    - Retrieves expanded legal context.\n",
    "    - Uses LLaMA to generate responses based on enriched knowledge.\n",
    "    \"\"\"\n",
    "    expanded_context = logical_expand_graph_rag(query)  # Step 1: Retrieve expanded legal context\n",
    "\n",
    "    # Step 2: Format the prompt for LLaMA\n",
    "    prompt = f\"\"\"\n",
    "    Given the following legal case references, provide a logically sound answer to the query.\n",
    "\n",
    "    Context:\n",
    "    {expanded_context}\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 3: Generate response using LLaMA\n",
    "    response = llm.generate([prompt], max_tokens=500)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "questions_df = pd.read_csv(\"../data/processed/Questions & Answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select test queries\n",
    "# test_questions = questions_df[\"question\"].sample(3).tolist()\n",
    "\n",
    "# # Iterate with tqdm progress bar\n",
    "# for i, test_query in enumerate(tqdm(test_questions, desc=\"Generating Responses\", unit=\"query\")):\n",
    "#     start_time = time.time()  # Track time per query\n",
    "\n",
    "#     print(f\"\\nüîπ **Test Query {i+1}:** {test_query}\")\n",
    "\n",
    "#     # Generate response\n",
    "#     response = generate_logically_reasoned_response(test_query)\n",
    "\n",
    "#     elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "\n",
    "#     print(f\"‚úÖ **LLaMA Response:**\\n{response}\")\n",
    "#     print(f\"‚è≥ Time Taken: {elapsed_time:.2f}s\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Responses Using ROUGE & BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation results\n",
    "evaluation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Responses:   0%|          | 0/4 [00:00<?, ?question/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating Responses:  25%|‚ñà‚ñà‚ñå       | 1/4 [36:59<1:50:57, 2219.04s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: How does the Supreme Court differentiate between an interlocutory order and a final order in civil appeals? | Time: 2198.12s | CPU: 41.15% | Memory: 73.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Responses:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [1:17:12<1:17:47, 2333.58s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: What is the legal significance of the immunity granted to Attorneys-at-Law regarding statements made in pleadings? | Time: 2394.36s | CPU: 56.35% | Memory: 94.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Responses:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [1:56:04<38:52, 2332.72s/question]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: In tax disputes, how should conflicts between two statutes be resolved? | Time: 2317.97s | CPU: 47.70% | Memory: 96.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [2:33:21<00:00, 2300.34s/question]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: How does Sri Lankan law determine the ‚Äòvalue‚Äô of shares for stamp duty purposes? | Time: 2218.91s | CPU: 52.35% | Memory: 95.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each question with a progress bar\n",
    "for _, row in tqdm(questions_df.iterrows(), total=len(questions_df), desc=\"Evaluating Responses\", unit=\"question\"):\n",
    "    question, ground_truth = row[\"question\"], row[\"answer\"]\n",
    "\n",
    "    # Ensure the inputs are valid strings\n",
    "    ground_truth = str(ground_truth) if ground_truth else \"\"\n",
    "    \n",
    "    # Measure system performance\n",
    "    start_time = time.time()\n",
    "    cpu_before = psutil.cpu_percent(interval=None)\n",
    "    memory_before = psutil.virtual_memory().percent\n",
    "\n",
    "    # Generate LLaMA response with logical reasoning\n",
    "    generated_response = generate_logically_reasoned_response(question)\n",
    "    \n",
    "    # Ensure the generated response is valid\n",
    "    generated_response = str(generated_response) if generated_response else \"\"\n",
    "\n",
    "    # Measure system performance after generation\n",
    "    cpu_after = psutil.cpu_percent(interval=None)\n",
    "    memory_after = psutil.virtual_memory().percent\n",
    "    response_time = time.time() - start_time\n",
    "\n",
    "    # Compute ROUGE scores if response is non-empty\n",
    "    if generated_response.strip() and ground_truth.strip():\n",
    "        rouge_scores = rouge.compute(predictions=[generated_response], references=[ground_truth])\n",
    "        bert_scores = bertscore.compute(predictions=[generated_response], references=[ground_truth], lang=\"en\")\n",
    "    else:\n",
    "        rouge_scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "        bert_scores = {\"precision\": [0.0], \"recall\": [0.0], \"f1\": [0.0]}\n",
    "\n",
    "    # Store results\n",
    "    evaluation_results.append({\n",
    "        \"question\": question,\n",
    "        \"response\": generated_response,\n",
    "        \"response_time\": response_time,\n",
    "        \"cpu_usage\": (cpu_before + cpu_after) / 2,\n",
    "        \"memory_usage\": (memory_before + memory_after) / 2,\n",
    "        \"rouge_scores\": rouge_scores,\n",
    "        \"bert_precision\": bert_scores[\"precision\"][0],\n",
    "        \"bert_recall\": bert_scores[\"recall\"][0],\n",
    "        \"bert_f1\": bert_scores[\"f1\"][0],\n",
    "    })\n",
    "\n",
    "    # Print log after each question\n",
    "    print(f\"‚úÖ Processed: {question} | Time: {response_time:.2f}s | \"\n",
    "          f\"CPU: {(cpu_before + cpu_after) / 2:.2f}% | \"\n",
    "          f\"Memory: {(memory_before + memory_after) / 2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation completed! Results saved to `evaluation_results.csv`.\n"
     ]
    }
   ],
   "source": [
    "# Save results for later analysis\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_csv(\"../data/processed/logical_reasoning/evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed! Results saved to `evaluation_results.csv`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä **Final Evaluation Results**\n",
      "‚è≥ Average Response Time: 2282.34s\n",
      "üíæ Average CPU Usage: 49.39%\n",
      "üñ•Ô∏è Average Memory Usage: 89.89%\n",
      "üìå Average ROUGE Scores: {'rouge1': 0.0024840936686022766, 'rouge2': 0.0005235178621953041, 'rougeL': 0.0014819822873653153}\n",
      "üîé Average BERT Precision: 0.7491\n",
      "üîé Average BERT Recall: 0.8424\n",
      "üîé Average BERT F1 Score: 0.7926\n"
     ]
    }
   ],
   "source": [
    "# Compute averages\n",
    "avg_response_time = evaluation_df[\"response_time\"].mean()\n",
    "avg_cpu_usage = evaluation_df[\"cpu_usage\"].mean()\n",
    "avg_memory_usage = evaluation_df[\"memory_usage\"].mean()\n",
    "\n",
    "# Compute average ROUGE scores\n",
    "avg_rouge = {metric: evaluation_df[\"rouge_scores\"].apply(lambda x: x[metric]).mean() for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]}\n",
    "\n",
    "# Compute average BERTScore (Precision, Recall, F1)\n",
    "avg_bert_precision = evaluation_df[\"bert_precision\"].mean()\n",
    "avg_bert_recall = evaluation_df[\"bert_recall\"].mean()\n",
    "avg_bert_f1 = evaluation_df[\"bert_f1\"].mean()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä **Final Evaluation Results**\")\n",
    "print(f\"‚è≥ Average Response Time: {avg_response_time:.2f}s\")\n",
    "print(f\"üíæ Average CPU Usage: {avg_cpu_usage:.2f}%\")\n",
    "print(f\"üñ•Ô∏è Average Memory Usage: {avg_memory_usage:.2f}%\")\n",
    "print(f\"üìå Average ROUGE Scores: {avg_rouge}\")\n",
    "print(f\"üîé Average BERT Precision: {avg_bert_precision:.4f}\")\n",
    "print(f\"üîé Average BERT Recall: {avg_bert_recall:.4f}\")\n",
    "print(f\"üîé Average BERT F1 Score: {avg_bert_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
