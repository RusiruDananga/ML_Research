{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import spacy\n",
    "from sympy import symbols, Implies\n",
    "import faiss\n",
    "import torch\n",
    "from langchain_ollama import OllamaLLM\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import evaluate\n",
    "import psutil\n",
    "from collections import Counter\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bc6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "qa_df = pd.read_csv(\"../data/processed/Questions & Answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd978d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunked legal cases dataset\n",
    "chunked_cases_df = pd.read_csv(\"chunked_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667dcb3e",
   "metadata": {},
   "source": [
    "### Multi-Hop Context via Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9333ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa55f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for all chunk texts\n",
    "chunked_cases_df['embedding'] = chunked_cases_df['text'].apply(lambda x: embedding_model.encode(x, convert_to_tensor=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148a8b8",
   "metadata": {},
   "source": [
    "### Function to Retrieve Top-5 Relevant Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de83e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(question, chunk_df, model, top_k=5):\n",
    "    question_emb = model.encode(question, convert_to_tensor=False)\n",
    "    chunk_embeddings = np.stack(chunk_df['embedding'].to_numpy())\n",
    "    \n",
    "    similarities = cosine_similarity([question_emb], chunk_embeddings)[0]\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    return \"\\n\\n\".join(chunk_df.iloc[top_indices]['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff75e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df['prompt'] = qa_df['question'].apply(\n",
    "    lambda q: f\"Context:\\n{get_relevant_chunks(q, chunked_cases_df, embedding_model)}\\n\\nQuestion:\\n{q}\\n\\nAnswer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67057a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LLaMA model\n",
    "llm = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c150fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f2149ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1480\\408248792.py:11: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  generated_answer = llm(prompt)\n"
     ]
    }
   ],
   "source": [
    "for idx, row in qa_df.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    true_answer = row['answer']\n",
    "\n",
    "    # Monitor CPU usage before\n",
    "    cpu_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # Measure response time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        generated_answer = llm(prompt)\n",
    "    except Exception as e:\n",
    "        generated_answer = f\"ERROR: {e}\"\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    # Monitor CPU usage after (sampling)\n",
    "    cpu_after = psutil.cpu_percent(interval=None)\n",
    "    avg_cpu_usage = (cpu_before + cpu_after) / 2\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"question\": row['question'],\n",
    "        \"true_answer\": true_answer,\n",
    "        \"generated_answer\": generated_answer,\n",
    "        \"response_time_sec\": response_time,\n",
    "        \"cpu_usage_percent\": avg_cpu_usage\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3377b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0f532",
   "metadata": {},
   "source": [
    "### Evaluate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5429f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def fuzzy_accuracy(pred, truth, threshold=0.8):\n",
    "    return SequenceMatcher(None, pred.lower(), truth.lower()).ratio() >= threshold\n",
    "\n",
    "results_df['is_accurate'] = results_df.apply(\n",
    "    lambda row: fuzzy_accuracy(row['generated_answer'], row['true_answer']), axis=1\n",
    ")\n",
    "accuracy = results_df['is_accurate'].mean()\n",
    "print(f\"Fuzzy Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "360fcdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-L: 0.2073\n"
     ]
    }
   ],
   "source": [
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def get_rouge_score(pred, truth):\n",
    "    return rouge.score(truth, pred)['rougeL'].fmeasure\n",
    "\n",
    "results_df['rougeL'] = results_df.apply(\n",
    "    lambda row: get_rouge_score(row['generated_answer'], row['true_answer']), axis=1\n",
    ")\n",
    "\n",
    "avg_rouge = results_df['rougeL'].mean()\n",
    "print(f\"Average ROUGE-L: {avg_rouge:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e1af58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m P, R, F1 \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m(\n\u001b[0;32m      2\u001b[0m     results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m      3\u001b[0m     results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m      4\u001b[0m     lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbertscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m F1\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      9\u001b[0m avg_bertscore \u001b[38;5;241m=\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbertscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score(\n",
    "    results_df['generated_answer'].tolist(),\n",
    "    results_df['true_answer'].tolist(),\n",
    "    lang='en',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "results_df['bertscore'] = F1.numpy()\n",
    "avg_bertscore = results_df['bertscore'].mean()\n",
    "print(f\"Average BERTScore F1: {avg_bertscore:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
