{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf8a9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import spacy\n",
    "from sympy import symbols, Implies\n",
    "import faiss\n",
    "import torch\n",
    "from langchain_ollama import OllamaLLM\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import evaluate\n",
    "import psutil\n",
    "from collections import Counter\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320610ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunked legal cases dataset\n",
    "chunked_cases_df = pd.read_csv(\"src/chunked_law_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9241d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "qa_df = pd.read_csv(\"../data/processed/Questions & Answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25629c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Cases Sample:\n",
      "    file_name  chunk_id                                               text\n",
      "0  012009.pdf         0  IN THE SUPREME COURT OF THE DEMOCRATIC SOCIALI...\n",
      "1  012009.pdf         1  Argued on :\\nDecided on:\\nJ.A.N. de Silva J\\nT...\n",
      "2  012009.pdf         2  the process culminated in the SLMC deciding to...\n",
      "3  012009.pdf         3  full knowledge and acquiescence of the leader ...\n",
      "4  012009.pdf         4  him that in order to be appointed as Chairman ...\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of both datasets to verify\n",
    "print(\"Chunked Cases Sample:\")\n",
    "print(chunked_cases_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6f45a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions & Answers Sample:\n",
      "                                            question  \\\n",
      "0  How does the Supreme Court differentiate betwe...   \n",
      "1  What is the legal significance of the immunity...   \n",
      "2  In tax disputes, how should conflicts between ...   \n",
      "3  How does Sri Lankan law determine the ‘value’ ...   \n",
      "\n",
      "                                              answer  \n",
      "0  The distinction is crucial for determining the...  \n",
      "1  Attorneys-at-law enjoy absolute immunity conce...  \n",
      "2  Conflicts between statutes, particularly in ta...  \n",
      "3  The Supreme Court emphasized that the \"value\" ...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nQuestions & Answers Sample:\")\n",
    "print(qa_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17daab5",
   "metadata": {},
   "source": [
    "### Prepare the Sample Data (3 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e93046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 3 rows from the QA dataset\n",
    "sample_qa_df = qa_df.head(3).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62a16f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Samples for Evaluation:\n",
      "                                            question  \\\n",
      "0  How does the Supreme Court differentiate betwe...   \n",
      "1  What is the legal significance of the immunity...   \n",
      "2  In tax disputes, how should conflicts between ...   \n",
      "\n",
      "                                              answer  \n",
      "0  The distinction is crucial for determining the...  \n",
      "1  Attorneys-at-law enjoy absolute immunity conce...  \n",
      "2  Conflicts between statutes, particularly in ta...  \n"
     ]
    }
   ],
   "source": [
    "# Display the selected samples\n",
    "print(\"Selected Samples for Evaluation:\")\n",
    "print(sample_qa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c4205",
   "metadata": {},
   "source": [
    "### Self-Consistency Decoding (Majority Voting for Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd2fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = OllamaLLM(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "178e1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query with Self-Consistency Decoding\n",
    "def generate_majority_answer(prompt, num_generations=5):\n",
    "    responses = []\n",
    "    cpu_start = psutil.cpu_percent(interval=None)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(num_generations):\n",
    "        response = llm(prompt)\n",
    "        responses.append(response.strip())\n",
    "\n",
    "    end_time = time.time()\n",
    "    cpu_end = psutil.cpu_percent(interval=None)\n",
    "    duration = end_time - start_time\n",
    "    avg_cpu = (cpu_start + cpu_end) / 2\n",
    "\n",
    "    # Get the most common response\n",
    "    final_answer = Counter(responses).most_common(1)[0][0]\n",
    "\n",
    "    return {\n",
    "        \"final_answer\": final_answer,\n",
    "        \"all_answers\": responses,\n",
    "        \"response_time_sec\": round(duration, 2),\n",
    "        \"cpu_usage_percent\": round(avg_cpu, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab6a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results list\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78046b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Sample 1...\n",
      "\n",
      "Evaluating Sample 2...\n",
      "\n",
      "Evaluating Sample 3...\n"
     ]
    }
   ],
   "source": [
    "# Loop through each sample\n",
    "for index, row in sample_qa_df.iterrows():\n",
    "    question = row['question']\n",
    "    gold_answer = row['answer']\n",
    "\n",
    "    print(f\"\\nEvaluating Sample {index+1}...\")\n",
    "    result = generate_majority_answer(prompt=question, num_generations=5)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"gold_answer\": gold_answer,\n",
    "        \"model_answer\": result[\"final_answer\"],\n",
    "        \"all_model_answers\": result[\"all_answers\"],\n",
    "        \"response_time_sec\": result[\"response_time_sec\"],\n",
    "        \"cpu_usage_percent\": result[\"cpu_usage_percent\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b96036a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53081173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Outputs with Metrics:\n",
      "                                            question  \\\n",
      "0  How does the Supreme Court differentiate betwe...   \n",
      "1  What is the legal significance of the immunity...   \n",
      "2  In tax disputes, how should conflicts between ...   \n",
      "\n",
      "                                        model_answer  response_time_sec  \\\n",
      "0  In civil appeals, the Supreme Court differenti...            2746.88   \n",
      "1  The immunity granted to attorneys-at-law regar...            2149.98   \n",
      "2  When dealing with tax disputes involving confl...            2112.04   \n",
      "\n",
      "   cpu_usage_percent  \n",
      "0               45.0  \n",
      "1               37.2  \n",
      "2               44.0  \n"
     ]
    }
   ],
   "source": [
    "# Preview the output\n",
    "print(\"\\nModel Outputs with Metrics:\")\n",
    "print(results_df[[\"question\", \"model_answer\", \"response_time_sec\", \"cpu_usage_percent\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b64aba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a85b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing scores\n",
    "accuracy_list = []\n",
    "rouge1_list = []\n",
    "rouge2_list = []\n",
    "rougeL_list = []\n",
    "bertscore_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99846b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row and compute metrics\n",
    "for index, row in results_df.iterrows():\n",
    "    gold = row['gold_answer'].strip()\n",
    "    pred = row['model_answer'].strip()\n",
    "\n",
    "    # Accuracy (exact match)\n",
    "    accuracy = int(gold.lower() == pred.lower())\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    # ROUGE scores\n",
    "    r_scores = rouge.score(gold, pred)\n",
    "    rouge1_list.append(r_scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_list.append(r_scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_list.append(r_scores[\"rougeL\"].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dc82525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d62e07ef2443cdb7eae2747bbb914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fba948be134191bf4990d31916ecdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 21.61 seconds, 0.14 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "# Compute BERTScore (batch for all)\n",
    "P, R, F1 = bert_score(\n",
    "    cands=results_df[\"model_answer\"].tolist(),\n",
    "    refs=results_df[\"gold_answer\"].tolist(),\n",
    "    lang=\"en\",\n",
    "    verbose=True\n",
    ")\n",
    "bertscore_list = F1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87536b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scores to DataFrame\n",
    "results_df[\"accuracy\"] = accuracy_list\n",
    "results_df[\"rouge1\"] = rouge1_list\n",
    "results_df[\"rouge2\"] = rouge2_list\n",
    "results_df[\"rougeL\"] = rougeL_list\n",
    "results_df[\"bertscore\"] = bertscore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d15d8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Scores Added:\n",
      "   accuracy    rouge1    rouge2    rougeL  bertscore\n",
      "0         0  0.236287  0.072034  0.139241   0.820778\n",
      "1         0  0.245968  0.068826  0.149194   0.847917\n",
      "2         0  0.230769  0.040650  0.121457   0.822547\n"
     ]
    }
   ],
   "source": [
    "# Preview results\n",
    "print(\"\\nEvaluation Scores Added:\")\n",
    "print(results_df[[\"accuracy\", \"rouge1\", \"rouge2\", \"rougeL\", \"bertscore\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
