{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking function\n",
    "def chunk_text(text, chunk_size=512, overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=count_tokens\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts):\n",
    "    return model.encode(texts, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve top 5 most similar chunks from the FAISS index\n",
    "def retrieve_top_5(query, index, embedding_df, model):\n",
    "    # Generate the embedding for the query\n",
    "    query_embedding = model.encode([query])[0]  # Get the query embedding\n",
    "    \n",
    "    # Search the FAISS index for the top 5 closest matches\n",
    "    D, I = index.search(np.array([query_embedding]), k=5)\n",
    "    \n",
    "    # Retrieve the top 5 results\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        result = embedding_df.iloc[idx]\n",
    "        results.append({\n",
    "            \"file_name\": result[\"file_name\"],\n",
    "            \"chunk_id\": result[\"chunk_id\"],\n",
    "            \"text\": result[\"text\"],\n",
    "            \"similarity_score\": D[0][list(I[0]).index(idx)]  # Distance (lower is more similar)\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response from Llama model\n",
    "def generate_response_with_llama(query, retrieved_docs, llm):\n",
    "    # Concatenate the retrieved documents into a prompt for the Llama model\n",
    "    context = \"\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Construct the prompt for Llama\n",
    "    prompt = f\"Based on the following legal documents, answer the query:\\n\\n{context}\\n\\nQuery: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Pass the prompt to the Llama model\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
